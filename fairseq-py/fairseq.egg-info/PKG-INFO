Metadata-Version: 2.1
Name: fairseq
Version: 0.4.0
Summary: Facebook AI Research Sequence-to-Sequence Toolkit
License: BSD License
        
        For fairseq software
        
        Copyright (c) 2017-present, Facebook, Inc. All rights reserved.
        
        Redistribution and use in source and binary forms, with or without modification,
        are permitted provided that the following conditions are met:
        
         * Redistributions of source code must retain the above copyright notice, this
            list of conditions and the following disclaimer.
        
         * Redistributions in binary form must reproduce the above copyright notice,
            this list of conditions and the following disclaimer in the documentation
               and/or other materials provided with the distribution.
        
         * Neither the name Facebook nor the names of its contributors may be used to
            endorse or promote products derived from this software without specific
               prior written permission.
        
        THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
        ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
        WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
        DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR
        ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
        (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
        LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
        ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
        (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
        SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
        
License-File: LICENSE
Requires-Dist: cffi
Requires-Dist: numpy
Requires-Dist: torch
Requires-Dist: tqdm

# seminaire_lemans
Documentation de mes experimentations sur fairseq

## Generer les BPE
1-	Learn BPE

subword-nmt learn-bpe -s 4000 < new_testament_ew_train.txt > bpe_codes_ew

subword-nmt learn-bpe -s 4000 < new_testament_fr_train.txt > bpe_codes_fr

2-	Apply BPE

subword-nmt apply-bpe -c bpe_codes_ew < new_testament_ew_train.txt > train.bpe.ew

subword-nmt apply-bpe -c bpe_codes_fr < new_testament_fr_train.txt > train.bpe.fr

subword-nmt apply-bpe -c bpe_codes_ew < new_testament_ew_test.txt > test.bpe.ew

subword-nmt apply-bpe -c bpe_codes_ew < new_testament_ew_dev.txt > dev.bpe.ew

subword-nmt apply-bpe -c bpe_codes_fr < new_testament_fr_test.txt > test.bpe.fr

subword-nmt apply-bpe -c bpe_codes_fr < new_testament_fr_dev.txt > dev.bpe.fr

## Preprocess Data for Fairseq

Fairseq requires data to be in a binary format:

preprocess.py --source-lang ew --target-lang fr --trainpref train.bpe --validpref dev.bpe --testpref test.bpe --destdir data-bin

## Train the model with conv architecture good for low resources

python3 train.py data-bin/ --arch fconv_iwslt_de_en     --optimizer adam     --lr 0.0005     --clip-norm 0.1     --dropout 0.3     --max-tokens 4000     --lr-scheduler reduce_lr_on_plateau     --lr-shrink 0.5     --criterion label_smoothed_cross_entropy     --label-smoothing 0.1     --max-epoch 20     --save-dir checkpoints/  --log-format json     --log-interval 10     --seed 42  --encoder-embed-dim 256  --encoder-layers 4     --decoder-embed-dim 256         --decoder-layers 4     --batch-size 32 --adam-betas "(0.9, 0.98)"



## Generate translation
python3 generate.py data-bin/ --path checkpoints/checkpoint_best.pt --batch-size 128 --beam 5 --remove-bpe

i resolve issue ofconv_tbc like: return torch.conv_tbc(input.contiguous(), self.weight, self.bias, self.padding[0]) link help: https://github.com/EdinburghNLP/XSum/issues/11

i had be inspired on this link: https://github.com/wengong-jin/fairseq-py and https://github.com/sliedes/fairseq-py

